\documentclass[12pt, letterpaper]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{comment}
\usepackage{caption} % for table captions

\usepackage[shortlabels]{enumitem} % for (a) (b) (c) enumerate use [(a)] like so:
%\begin{enumerate}[(a)]
\usepackage{bm}%bold symbols in math mode
\usepackage{amsmath}%allows text in math mode e.g. \text{}

\usepackage{listings} % Typeset Python
%Here is an example of how to do that
%\begin{lstlisting}[language=Python, basicstyle=\tiny]
%#Here is some test Python
%def yay():
%    print("hi")
%\end{lstlisting}

\usepackage{graphicx} % for images
%\includegraphics{uploaded-file-name}

\usepackage{float} % supposedly will allow the use of [H] for table and figure positioning

\title{COSC 5010-03 Practical Machine Learning Fall 2023 Hyperparameter Optimization Report}
\author{Michael Elgin}
\date{October 31, 2023}

\begin{document}

\maketitle

\section{Introduction} %What problem are you solving, how are you going to solve it.

Various machine learning algorithms can not only be trained on data but also tuned based on hyperparameters they offer. This can improve a model's ability to generalize to new data. This exercise compares the performance of various algorithms as their hyperparameters are adjusted. The wine quality dataset\footnote{https://archive.ics.uci.edu/dataset/186/wine+quality} is used.

\section{Dataset Description} %Describe the data you're using, e.g. how many features and observations, what are you predicting, any missing values, etc.

The wine quality dataset is standard tabular data. There are 2 datasets for each color of red and white. Both contain 11 features, all of which are continuous. The target is a discrete value which is the assigned quality of the wine. For the regression tasks, the white wine dataset is used to evaluate models. For classification tasks, both datasets are concatenated, with the target being changed to be the color of the wine, with 0 being assigned to red and 1 to white.

\section{Experimental Setup} %What specifically are you doing to solve the problem, i.e. what programming languages and libraries, how are you processing the data, what machine learning algorithms are you considering, how are you evaluating them, etc.

All computation is done with the Python programming language. Scikit-learn is used to construct models. Pandas is used to load and preprocess the data..

For regression, performance is the mean absolute percentage error, often graphed as its negative (then meaning more is better). This is defined by taking the difference between the predicted value of the model and the actual value, then dividing by the actual value. Then the mean of all of those is taken. More formally:

$$
    GE_{Regression\ model}(\hat{f}, \bm{X}_{test}, \bm{y}_{test}) = \frac{1}{|\bm{X}_{test}|} \sum_{i=1}^{|\bm{X}_{test}|} \frac{\hat{f}(\bm{X}_{test,i} - \bm{y}_{test,i}) \cdot 100}{\bm{X}_{test,i}}
$$

For classification, the performance metric is standard accuracy:

$$
    GE_{Classification\ model}(\hat{f}, \bm{X}_{test}, \bm{y}_{test}) = \frac{\sum_{i=1}^{|\bm{X}_{test}|}l_{0,1}(\hat{f}(\bm{X}_{test,i}), \bm{y}_{test,i})}{|\bm{X}_{test}|}
$$

$$
Acc_{Classification\ model} = (1 - GE_{Classification\ model}) * 100
$$

For all models, grid-search is used for trying hyperparameter configurations. For hyperparameters that are continuous in nature, the grid is exponential in fashion, meaning exponents for $2^x$ are tried. This allows for a much wider exploration of the hyperparameter space given realistic time constraints, since adjusting hyperparameters in a linear fashion would space all configurations too closely together.

Grid search does not use nested resampling. The Bayesian optimization by itself does not use nested resampling. It uses a default of 3-fold inner cross-validation. As an additional layer to do nested resampling, 3-fold outer cross-validation wraps this process, and the average of those scores is reported in table \ref{perf_measures_cls}.

The first section is regression models. Model 1 is a decision tree. The hyperparameters considered for this are max depth for the tree and minimum samples required for a split. All hyperparameters explored can only have positive numbers. The minimum amount of samples must be 2, hence the first exponent starts at 1.

$$
\text{max depth} = 2^x, x \in [0,7]
$$

$$
\text{min samples} = 2^x, x \in [1,15]
$$

The second model is a random forest, which uses the same hyperparamters as the tree but also adds in the third hyperparameter of the amount of trees in the forest.

$$
\text{max depth} = 2^x, x \in [0,4]
$$

$$
\text{min samples} = 2^x, x \in [1,4]
$$

$$
\text{number of trees} = 2^x, x \in [0,7]
$$

The second section is classification models. Model 1 is a support vector classifier. Its first hyperparmeter is ``C", which is inverse regularization strength. The second hyperparameter is the kernel type, which is either poly (polynomial) or rbf (radial basis function).

$$
\text{C} = 2^x, x \in [0,15]
$$

$$
\text{kernel type} \in \{\text{poly, rbf}\}
$$

Model 2 is logistic regression, which uses the following hyperparameter settings.

$$
\text{C} = 2^x, x \in [0,8]
$$

$$
\text{penalty type} \in \{\text{l1, l2}\}
$$

Model 3 is a decision tree classifier, which uses the same hyperparmeter settings as in regularization.

$$
\text{max depth} = 2^x, x \in [0,7]
$$

$$
\text{min samples} = 2^x, x \in [1,15]
$$

Model 4 is a K-nearest neighbor classifier, whose hyperparameters are the amount of neighbors to consider and the distance metric.

$$
\text{number of neighbors} = 2^x, x \in [0,7]
$$

$$
\text{distance metric} \in \{\text{l1, l2}\}
$$

Model 5 is a random forest classifier, which uses the same hyperparameters as in regression.

$$
\text{max depth} = 2^x, x \in [0,4]
$$

$$
\text{min samples} = 2^x, x \in [1,4]
$$

$$
\text{number of trees} = 2^x, x \in [0,7]
$$

For each model, Bayesian optimization will be given the same budget (the same amount of hyperparameter configurations to try) as was had for grid-search.

\section{Results} %Description of what you observed, including plots.

\subsection{Tables of best hyperparameters}

The following tables show the best configurations found from grid-search, and Bayes-search. These do not always match the highest points on the plots of the averages because the table reports the best \emph{combination} of hyperparameters from grid-search. The first value is from grid-search, prefixed by ``G:", and the second is from Bayesian optimization, prefixed by ``B:". Note that results from runs of Bayesian optimization may vary if the random\_state seed is changed from 0.

\begin{table}[H]
\centering
\caption{Regression models' best hyperparameters}
\label{reg_table}
\begin{tabular}{c|c|c|c} % l for left-aligned column, c for centered columns
                & Max Depth     & Min Samples for Split & Number of Trees \\ \hline
Decision Tree   & G:8, B:44            & G:512, B:582                  &   \\
Random Forest   & G:8, B:8             & G:8, B:8                    & G:64, B:58 \\
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Classification models' best hyperparameters}
\label{cls_table 1}
\begin{tabular}{c|c|c|c} % l for left-aligned column, c for centered columns
                            & C         & Kernel Type   & Penalty Type \\ \hline
Support Vector Classifier   & G:16384, B:16384     & G:rbf, B:rbf \\
Logistic Regression         & G:64, B:107        &               & G:l2, B:l2 \\
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Classification models' best hyperparameters}
\label{cls_table 2}
\begin{tabular}{c|c|c|c} % l for left-aligned column, c for centered columns
                & Number of Neighbors & Distance Metric\\ \hline
K-Nearest Neighbor & G:8, B:12 & G:l1, B:l1 \\
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Classification models' best hyperparameters}
\label{cls_table 3}
\begin{tabular}{c|c|c|c} % l for left-aligned column, c for centered columns
                & Max Depth     & Min Samples for Split     & Number of Trees \\ \hline
Decision Tree   & G:32, B:58 & G:2, B:2 \\
Random Forest   & G:8, B:7 & G:2, B:8 & G:32, B:64 \\
\end{tabular}
\end{table}

\subsection{Tables of performance measures for grid-search and Bayesian optimization}

Table \ref{perf_measures_reg} compares the best found $GE_{Regression\ model}$ for the regression models.

\begin{table}[H]
\centering
\caption{Regression MAPE comparison for grid-search and Bayesian optimization (less is better)}
\label{perf_measures_reg}
\begin{tabular}{c|c|c|c} % l for left-aligned column, c for centered columns
                            & Grid Search     & Bayesian Optimization & Budget (iterations) \\ \hline
Decision Tree Regressor     & 10.70\% & 10.34\% & 98\\
Random Forest Regressor     & 10.14\% & 9.47\% & 84\\
\end{tabular}
\end{table}

Table \ref{perf_measures_cls} compares the best found $Acc_{Classification\ model}$ for the classification models.

\begin{table}[H]
\centering
\caption{Classification accuracy comparison for grid-search and Bayesian optimization (more is better)}
\label{perf_measures_cls}
\begin{tabular}{c|c|c|c} % l for left-aligned column, c for centered columns
                            & Grid Search     & Bayesian Optimization & Budget (iterations) \\ \hline
Support Vector Classifier   & 98.815\% & 98.861\% & 30\\
Logistic Regression         & 98.815\% & 98.846\% & 16\\
K-Nearest Neighbors         & 94.582\% & 94.844\% & 14\\
Decision Tree Classifier    & 98.107\% & 98.199\% & 98\\
Random Forest Classifier    & 99.338\% & 99.369\% & 84\\
\end{tabular}
\end{table}

Based on the tables, it is not clear whether grid-search or Bayesian optimization is ``better" than the other. Although in all cases Bayesian optimization got a better number, the differences were trivial.

\section{Code} %Add the code you've used as a separate file.

The associated code is in HPO.ipynb

\section{Appendix}
\subsection{Plotting hyperparam performance across the grid}

For each hyperparameter config tested, the score is gathered at each cross validation split as well as the average of those scores. When hyperparameter values are judged here in this appendix, it is based on the average of the averages of all times they were used. For example, if hypothetical hyperparameter $x = a$ (along with other hyperparameters and their values) is being evaluated, it is cross validated to produce one average, and then the true performance of value $a$ is considered as the average of all the average scores whenever $x$ was $a$ even as other hyperparameter values were different.

\newcommand{\myscale}{0.4}

Plots for Decision Tree Regressor hyperparameters:

\includegraphics[scale=\myscale]{decision_tree_regressor_Max Depth.png}

\includegraphics[scale=\myscale]{decision_tree_regressor_Min Samples for Split.png}

Plots for Random Forest Regressor hyperparameters:

\includegraphics[scale=\myscale]{random_forest_regressor_Max Depth.png}

\includegraphics[scale=\myscale]{random_forest_regressor_Min Samples for Split.png}

\includegraphics[scale=\myscale]{random_forest_regressor_Number of Trees.png}

Plots for Support Vector Classifier hyperparameters:

\includegraphics[scale=\myscale]{svc_C Value.png}

\includegraphics[scale=\myscale]{svc_Kernel Type.png}

Plots for Logistic Regression hyperparameters:

\includegraphics[scale=\myscale]{logistic_regression_C Value.png}

\includegraphics[scale=\myscale]{logistic_regression_Penalty Type.png}

Plots for K-Nearest Neighbor hyperparameters:

\includegraphics[scale=\myscale]{knn_Number of Neighbors.png}

\includegraphics[scale=\myscale]{knn_Metric.png}

Plots for Decision Tree Classifier:

\includegraphics[scale=\myscale]{decision_tree_classifier_Max Depth.png}

\includegraphics[scale=\myscale]{decision_tree_classifier_Min Samples for Split.png}

Plots for Random Forest Classifier:

\includegraphics[scale=\myscale]{random_forest_classifier_Max Depth.png}

\includegraphics[scale=\myscale]{random_forest_classifier_Min Samples for Split.png}

\includegraphics[scale=\myscale]{random_forest_classifier_Number of Trees.png}

Combination plots follow\footnote{
Note that the time cost of grid-search kept more values from being explored.
}.

\includegraphics[scale=\myscale]{combo_C.png}

\includegraphics[scale=\myscale]{combo_max_depth.png}

\includegraphics[scale=\myscale]{combo_min_samples_split.png}

\includegraphics[scale=\myscale]{combo_n_estimators.png}

For some hyperparameters, similar trajectories were observed between models. For others there was no reliable pattern across the space of values.

\end{document}